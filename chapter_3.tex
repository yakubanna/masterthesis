\chapter{Управление с прогнозирующими моделями в model-based обучении с подкреплением}\label{chap4}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Марковский процесс принятия решений для в рамках управления с прогнозирующими моделями}\label{1sec:optimal-control}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


В рамках управления с прогнозирующими моделями в задачах оптимального управления с ограничениями марковский процесс принятия решений несколько трансформируется. Кроме перечисленных в главе 2 характеристик (состояние среды, множество возможных действий, модель среды, функции наград) к нему добавляется функция стоимости $c: S \to \{0, 1\}$. Она является индикатором, где 0 значит, что ограничения выполнены, а 1 -- нарушены. 

В рамках исследуемой задачи мы считаем, что модель среды и функция стоимости неизвестны и должны быть найдены на основе собранных данных. Пусть $J_r (\pi)$ обозначает ожидаемую доходность при политике $\pi$ относительно функции вознаграждения $r$, а $J_c (\pi)$ -- доходность при политике $\pi$ относительно функции стоимости $c$:

$$J_r(\pi) = E_{\pi}[\sum\lim_{t=0}^{T-1} r(s_{t+1})],$$
$$J_c(\pi) = E_{\pi}[\sum\lim_{t=0}^{T-1} c(s_{t+1})],$$
где $T$ -- это горизонт планирования. 

Как было сказано выше, основная цель -- максимизировать ожидаемую доходность. При наложенных ограничениях задача состоит в том, чтобы максимизировать ожидаемую доходность  относительно вознаграждения и ограничить ожидаемую доходность относительно функции стоимости. В общем виде это может быть записано следующим образом:

$$\pi^* = \arg \max_{\pi} J_r(\pi),$$
$$J_c(\pi^*) \leq d, $$
где $d$ -- заданный лимит на количество нарушений ограничений. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Управление с прогнозирующими моделями (MPC)}\label{1sec:optimal-control}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Напомним, что как следует из главы 1 в основе MPC лежат два основных принципа:
\begin{enumerate}
	\item Использовать динамическую модель для предсказания состояния системы и улучшения предсказания для принятия оптимального решения -- \textit{управления} в текущий момент времени.
	
	\item Использовать предыдущие состояния системы для инициализации состояния динамической модели.
\end{enumerate}

Также напомним, что MPC применяется для решения задач стабилизации и в этом случае выбираются специальные критерии качества, которые на оптимальном значении представляет собой функцию Лапунова для стабилизируемой системы. Прогнозируемая задача оптимального управления в данном случае -- задача на минимум.

В терминах исследуемой задачи цель MPC — максимизировать суммарную награду, выбирая последовательность действий $U=(a_0, \dots, a_T)$, где $T$ -- это горизонт планирования. За один шаг к системе применяется оптимальное управление (действие) и  получаются новые наблюдения, после чего заново пересчитываются следующие оптимальные действия. Таким образом находясь в состоянии $s_t$ в момент времени $t$ целью MPC является найти

\begin{equation}
	U = \arg \max_{a_0, a_1, \dots, a_T} [ \mathbb{E}  \sum \limits_{t=0}^{T} \gamma^t r(s_{t+1})] , 
\end{equation}
$$s_{t+1} = f(s_t, a_t),$$
$$c(s_{t+1}) = 0,$$
$$\forall t \in {0, 1, \dots, T-1}. $$


Визуально представить себе работу управления по прогнозирующей модели можно так, как показано на рисунке ~\ref{fig:mpc-ex}. \newpage

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.7]{mpc_ex.png}
	\caption{МРС}\label{fig:mpc-ex}
\end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Model-based обучение с подкреплением}\label{1sec:optimal-control}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

В данный момент существует два основных подхода в обучении с подкреплением: model-based и model-free. Основное их отличием является взаимодействие и представление о среде. Model-free алгоритмы обучаются на собственном опыте при запуске в реальной среде для обучения (компьютерные игры, реальный мир). Подход model-based же контактирует с реальной средой намного меньше и на этапе обучения старается заменить ее собственным аналогом среды (моделью). Таким образом одной из основных задач model-based алгоритмов является построение качественной модели среды.

Как было сказано в разделе выше, в рамках исследуемой задачи один из способов решения будет основан на отсутствии знаний о среде. Таким образом мы должны получить представление о ней на основе генерации выборок из реальной среды. В качестве представления среды будем использовать небольшую нейронную сеть, изображенную на рисунке ~\ref{fig:nn-new}. \newpage

 \begin{figure}[h]
 	\centering
 	\includegraphics[scale=0.65]{nn_new.jpeg}
 	\caption {Модель среды}
 	\label{fig:nn-new}
 \end{figure}

Процесс обучения модели и работы управления с прогнозирующими моделями представлен на рисунке ~\ref{fig:mpc-m}. \newpage
 
 
 \begin{figure}[h]
 	\centering
 	\includegraphics[scale=0.6]{mpc_model.png}
 	\caption {Модель обучения MPC}
 	\label{fig:mpc-m}
 \end{figure}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Метод кросс-энтропии для задач оптимизации}\label{1sec:optimal-control}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Метод кросс-энтропии (CEM) это метод стохастической оптимизации, основанный на выборке \cite{cem2}. С недавнего времени он часто используется в задачах обучения с подкреплением. В рамках данного метода предполагается, что $T$-размерное решение $U \in R^T$ получено из $T$-размерного факторизированного многомерное распределение Гаусса с параметром $\theta$.

Тогда мы имеем выборку $U \sim N(\theta)$, где $\theta = (\mu, \Sigma)$, $\mu$ --  $T$-вектор, а $\Sigma$ -- диагональная $T x T$ матрица ковариаций.

Основная идея метода заключается в том, чтобы генерировать решения итеративно из распределения, которое близко к предыдущим сгенерированным результатам с высокой наградой. Алгоритм останавливается, если достигнуто максимальное количество итераций или $|\Sigma| > \epsilon$, где $\epsilon$ -- заданный фиксированный порог на матрицу ковариаций. 

Одним из способов решения исследуемой задачи оптимизации (3.1) является робастный метод кросс-энтропии (RCE), основанный на методе кросс-энтропии. Он базируется на  методе сэмплирования траекторий (СТ)  \cite{cem1} для оценки вознаграждения и стоимости нарушения ограничений. Определим решение как последовательность действий размера 
горизонта планирования $T$:
 $$U = (a_0, a_1, ..., a_{T −1}).$$  
 
При начальном состоянии $s_0$ , модели среды $f_{\theta}(s_t, a_t)$, функции стоимости  $c(s) \in {0, 1}$, мы можем оценить накопленное вознаграждение следующим образом:
$$r(X, s_0) = \sum\limits_{t=0}^T \gamma^t r(s_{t+1}),$$
$$c(X, s_0) = \sum\limits_{t=0}^T \beta^t c(s_{t+1}) .$$
 где $s_{t+1} = f_{\theta}(s_t, a_t), \forall  t \in \{0, \dots, T-1\}$, $\gamma$ и $\beta$ -- дисконтирующие множители. 

Награда $r(s)$ может быть либо предопределенной, заранее функцией, либо изучена параллельно с моделью.  

Интуитивное представление о работе данного метода на примере задачи о роботе показано на рисунке ~\ref{fig:cross-ent}. Точки на синей линии и точки на оранжевой линии представляют собой две реальные траектории, эллипсы
представляют собой неопределенность прогноза модели среды на основе наблюдений и последовательности действий. На рисунке видно, что награда за траекторию B должна быть выше, чем за траекторию A, потому что выбор B приведет к в достижению цели быстрее. Однако траектория A предпочтительнее, потому что робот, следующий по траектории B, может пройти
через пламя и нарушить ограничения безопасности.


\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{cem_alg.png}	
	\caption {Метод кросс-энтропии}
	\label{fig:cross-ent}
\end{figure}


Без сэмплирования траекторий, траектория B потенциально может быть предсказана как безопасный маршрут из-за ошибки прогноза модели.  С СТ,
оценка неопределенности нашей модели имеет небольшой
шанс охватить небезопасную зону, поэтому траектория B будет
классифицироваться как небезопасная. Поскольку СТ оценивает стоимость
траектории при наихудшем сценарии среди всех отобранных маршрутов, он более надежен, когда прогноз модели среды не очень точный.


Напомни, что функция вознаграждения как в формуле (4): $R(x, s) : S \to R$. Обозначим функцию плотности распределения как $p(U, \theta)$. 
Тогда алгоритм состоит из следующих шагов: 
\begin{enumerate}
	\item Выбираем набор решений.
	\item Считаем для него функции $R, c$.
	\item Проверяем достижимы ли данные решения. 
	\item Пересчитываем параметры распределения на основе лучших $k$ решений.
\end{enumerate}

Псевдокод описанного выше алгоритма выглядит следующим образом: 
\begin{algorithm}
	\caption{Метод робастной кросс-энтропии}\label{cem}
	\begin{algorithmic}[1]
		\State{Начальная инициализация параметра распределения $\theta$, размерность выборки $T$, количества отбираемых элементов из выборки $k$, начального состояния $s_0$.}
		\State{Генерация выборки $X$  с высокой наградой}
		
		\While{Критерий остановки  не выполнен}
		\State{Сгенерировать $N$ примеров из исходного распределения $U_1, U_2, \dots, U_N  \sim N(\theta)$.}
		\State{Для каждого элемента выборки $U_i$ вычислить $R(U_i, s_0)$ и $c(U_i, s_0)$.}
		\State{Выбрать подходящее подмножество элементов $\Omega \in \{U_i \}_{i=1}^N$ не нарушающих ограничения на основе функции стоимости.}
		\If{$\Omega = \emptyset$ -- пустое множество} 
		\State{Отсортировать $\{U_i\}_{i=1}^N$ в порядке убывания по функции стоимости; задать множество $\Delta_k$ -- из топ $k$ элементов после сортировки}
		\Else{}
		\State{Отсортировать множество $\Omega$ в порядке убывания по награде. Задать множество $\Delta_k$ из первых $k$ элементов, если $|\Omega| > k$, или $\Delta_k = \Omega$}.
		\EndIf
		\State{Обновить $\theta$ с помощью метода максимального правдоподобия: $\theta = arg \max_{\theta} \prod_{U \in \Delta_k} p(U, \theta)$.}
		\EndWhile
		\State{Вернуть $U^*$ с самой высокой наградой из $\Delta_k$.}
	\end{algorithmic}
\end{algorithm}
\bigskip

Для исследуемой задачи мы будем использовать модернизированный робастный метор кросс-энтропии. Вместо минимизации функции стоимости мы минимизируем максимум функции стоимости при подсчете подходящего подмножества. Кроме того основная цель -- оптимизация параметров для получения оптимальной политики в model-free обучении с подкреплением. Мы же напрямую оптимизируем горизонт планирования в model-based обучении с подкреплением. Псевдокод для нашего алгоритма можно увидеть ниже: \newpage

\begin{algorithm}
	\caption{Метод робастной кросс-энтропии}\label{cem}
	\begin{algorithmic}[1]
		\State{Начальная генерация выборки $D$, инициализация параметра $p$.}
		
		\While{Критерий остановки  не выполнен}
		\State{Тренировать модель среды $f$ и функцию стоимости $c$, имея выборку.}
		\For{t=0 до конца длины эпизода }
		\State{Получить состояние среды $s_t$}
		\State{Оптимизировать действие согласно алгоритму 1: \\ $\{a_{i^*}\}_{i=t}^{t+T} = RCE(p, s_t)$}
		\State{Выбрать действие и подать его в систему }
		\State{Получить из среды следующее состояние $s_{t+1}$ и функцию стоимости $c(s_{t+1})$}
		\State{Обновить выборку $D=D \bigcup \{ s_t, a_t, s_{t+1}, c(s_{t+1})\}$.}
		
		\EndFor
		\EndWhile
		\State{Вернуть $X^*$ с самой высокой наградой из $\Delta_k$.}
	\end{algorithmic}
\end{algorithm}

%В обучении с подкреплением же это означает наличие представления о среде $M$ для MDP $(S, A, P, \gamma,  R)$. А именно наличие версии модели окружающей среды в момент времени $i$ для Марковского процесса принятия решений.  

%Таким образом при известном пространстве состояний $S$ и вероятности переходов $A$, модель $M_i$ будет выглядеть как $M_i(S, A, P_i,\gamma,  R_i)$. И это значит, что, согласно модели вероятность перехода из состояния $S$ в $S ‘$ после выполнения действия $A$ равна $P_i (S' | S, A)$, аналогично вознагражение, полученное в результате действия $A$  в состоянии $S$  равно $R_i (r '| S,  A)$.


